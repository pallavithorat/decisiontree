{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Trees — Student Lab\n",
        "\n",
        "We start using **sklearn** in Week 4, but you’ll still implement core pieces from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Tree = It's ML algorithm that splits data into branches to make predictions based on feature values.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic dataset\n",
        "We’ll create a non-linear boundary dataset to show how trees fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n"
          ]
        }
      ],
      "source": [
        "def make_nonlinear(n=400):\n",
        "    X = rng.uniform(-2, 2, size=(n, 2))\n",
        "    # circle boundary\n",
        "    r = np.sqrt(X[:,0]**2 + X[:,1]**2)\n",
        "    y = (r < 1.0).astype(int)\n",
        "    # add noise\n",
        "    flip = rng.random(n) < 0.05\n",
        "    y[flip] = 1 - y[flip]\n",
        "    return X, y\n",
        "\n",
        "X, y = make_nonlinear()\n",
        "n = X.shape[0]\n",
        "idx = rng.permutation(n)\n",
        "tr = idx[: int(0.7*n)]\n",
        "va = idx[int(0.7*n):]\n",
        "Xtr, ytr = X[tr], y[tr]\n",
        "Xva, yva = X[va], y[va]\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Impurity\n",
        "\n",
        "### Task 1.1: Gini impurity\n",
        "\n",
        "# TODO: implement gini(y)\n",
        "# HINT: p_k = count_k / n; gini = 1 - sum(p_k^2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: gini_pure0\n",
            "OK: gini_half\n"
          ]
        }
      ],
      "source": [
        "def gini(y):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    if y.size == 0:\n",
        "        return 0.0\n",
        "    p1 = y.mean()\n",
        "    p0 = 1 - p1\n",
        "    return float(1 - p0**2 - p1**2)\n",
        "\n",
        "check('gini_pure0', abs(gini(np.zeros(10, dtype=int))) < 1e-12)\n",
        "check('gini_half', abs(gini(np.array([0,1]*5)) - 0.5) < 1e-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Entropy\n",
        "\n",
        "# TODO: implement entropy(y)\n",
        "# HINT: entropy = -sum p log2 p (use eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# entropy : measure of uncertainty in a set of labels\n",
        "# ex : how mixed are the labels?\n",
        "# if low entropy, that means most labels are the same\n",
        "# if high entropy, that means labels are evenly mixed\n",
        "# how good the split is? is done by gini or entropy\n",
        "def entropy(y):\n",
        "    # TODO\n",
        "    y = np.asarray(y, dtype=int)\n",
        "    if y.size == 0:\n",
        "        return 0.0\n",
        "    p1 = y.mean()\n",
        "    p0 = 1 - p1\n",
        "    eps = 1e-12\n",
        "    return float(-(p0*np.log2(p0 + eps) + p1*np.log2(p1 + eps)))\n",
        "\n",
        "print(abs(entropy(np.zeros(10, dtype=int))) < 1e-9)\n",
        "    \n",
        "\n",
        "# check('entropy_pure0', abs(entropy(np.zeros(10, dtype=int))) < 1e-12)\n",
        "# check('entropy_half', abs(entropy(np.array([0,1]*5)) - 1.0) < 1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Best split (decision stump)\n",
        "\n",
        "### Task 2.1: Evaluate impurity after threshold split\n",
        "\n",
        "Split rule: go left if X[:,j] <= t else right.\n",
        "Return weighted impurity and information gain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_indices(X, j, t):\n",
        "    left = np.where(X[:, j] <= t)[0]\n",
        "    right = np.where(X[:, j] > t)[0]\n",
        "    return left, right\n",
        "\n",
        "def info_gain(y, y_left, y_right, criterion='gini'):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "# quick sanity\n",
        "y0 = np.array([0,0,1,1])\n",
        "gain = info_gain(y0, np.array([0,0]), np.array([1,1]), criterion='gini')\n",
        "check('gain_positive', gain > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Find best (feature, threshold)\n",
        "\n",
        "# TODO: implement best_split(X, y)\n",
        "# HINT: thresholds from sorted unique feature values midpoints\n",
        "\n",
        "**FAANG gotcha:** if a split makes an empty child, skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def best_split(X, y, criterion='gini'):\n",
        "    # TODO: return (best_j, best_t, best_gain)\n",
        "    ...\n",
        "\n",
        "j, t, gain = best_split(Xtr, ytr)\n",
        "print('best', j, t, gain)\n",
        "check('gain_nonneg', gain >= 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3: Train a stump and evaluate\n",
        "\n",
        "Use best_split to build a stump that predicts majority class on each side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stump_predict(X_train, y_train, X_test, criterion='gini'):\n",
        "    # TODO\n",
        "    ...\n",
        "\n",
        "def accuracy(y, yhat):\n",
        "    return float(np.mean(y == yhat))\n",
        "\n",
        "yhat_tr = stump_predict(Xtr, ytr, Xtr)\n",
        "yhat_va = stump_predict(Xtr, ytr, Xva)\n",
        "print('stump train acc', accuracy(ytr, yhat_tr))\n",
        "print('stump val acc', accuracy(yva, yhat_va))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — sklearn DecisionTreeClassifier (sanity check)\n",
        "\n",
        "### Task 3.1: Train trees with different max_depth\n",
        "\n",
        "# TODO: train sklearn tree and compare train/val accuracy for depth in [1,2,3,5,None].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "depths = [1,2,3,5,None]\n",
        "for md in depths:\n",
        "    clf = DecisionTreeClassifier(max_depth=md, random_state=0)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    tr_acc = clf.score(Xtr, ytr)\n",
        "    va_acc = clf.score(Xva, yva)\n",
        "    print('max_depth', md, 'train', tr_acc, 'val', va_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Failure mode: leakage\n",
        "\n",
        "### Task 4.1: Create a leaky feature\n",
        "Add a feature that is directly derived from y and watch validation accuracy jump.\n",
        "\n",
        "**Explain:** why do trees exploit leakage aggressively?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xtr_leak = np.hstack([Xtr, ytr.reshape(-1,1)])\n",
        "Xva_leak = np.hstack([Xva, yva.reshape(-1,1)])\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "clf.fit(Xtr_leak, ytr)\n",
        "print('val acc with leakage', clf.score(Xva_leak, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Stump implemented\n",
        "- sklearn depth sweep shown\n",
        "- Leakage demo explained\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
